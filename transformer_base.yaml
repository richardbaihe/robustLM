target:
  service: amlk8s
  name: itpeastusv100cl
  vc: resrchvc
  # itpeastusv100cl
  # service: aml
  # name: canada24

environment:
  # image: nvidia/pytorch:20.12-py3
  # registry: nvcr.io
  image: richardbaihe/pytorch:azure
  setup:
    - export MKL_SERVICE_FORCE_INTEL=1 
    - python -m nltk.downloader punkt wordnet
    
code:
  local_dir: $CONFIG_DIR/src
  
data:
  local_dir: $CONFIG_DIR/data/wikitext-103
  remote_dir: data/wikitext-103

# list of jobs to run, we run 2 jobs in this example
jobs:
- name: train_small_32
  sku: G4
  command:
  - python -m torch.distributed.launch 
    --nproc_per_node 4 
    --nnodes 1 
    --node_rank 0 
    --master_addr 127.0.0.1 
    --master_port 16010
    train.py 
    --model_size small
    --job_name trans_small_32
    --batch_size 32
    --max_step 200000
    --mix_vocab
    --cl_annealing 0
    --dataset wt103
    --mem_len 0
    --adaptive
    --fp16
    --dynamic-loss-scale
    --multi_gpu
    --cuda
    --pt
    --do_train
    --do_test
- name: train_base_8_0.64
  sku: G4
  command:
  - python -m torch.distributed.launch 
    --nproc_per_node 4 
    --nnodes 1 
    --node_rank 0 
    --master_addr 127.0.0.1 
    --master_port 16010
    train.py 
    --job_name train_base_8_0.64
    --max_step 200000
    --batch_size 8
    --model_size base
    --cl_annealing 0.64
    --wn_layer 5
    --mix_vocab
    --input_root
    --dataset wt103
    --mem_len 0
    --adaptive
    --fp16
    --dynamic-loss-scale
    --multi_gpu
    --cuda
    --pt
    --do_train
    --do_test
- name: trans_small_0.64
  sku: G4
  command:
  - python -m torch.distributed.launch 
    --nproc_per_node 4 
    --nnodes 1 
    --node_rank 0 
    --master_addr 127.0.0.1 
    --master_port 16010
    train.py 
    --job_name small_0.64
    --cl_annealing 0.64
    --max_step 100000
    --batch_size 64
    --model_size small
    --mix_vocab
    --input_root
    --dataset wt103
    --mem_len 0
    --adaptive
    --fp16
    --dynamic-loss-scale
    --multi_gpu
    --cuda
    --pt
    --do_train
    --do_test

search:
  job_template:
    name: search_{experiment_name:s}_{auto:s}
    sku: G4
    command:
    - python -m torch.distributed.launch
      --nproc_per_node 4 
      --nnodes 1 
      --node_rank 0 
      --master_addr 127.0.0.1 
      --master_port 16010
      train.py 
      --job_name search_base_bsz_16_pf_{pacing_function}_pu_{pacing_unit}_{a}_{b}
      --pacing_function {pacing_function}
      --pacing_unit {pacing_unit}
      --a {a}
      --b {b}
      --model_size base
      --max_step 200000
      --batch_size 16
      --dataset wt103
      --mix_vocab
      --input_root
      --mem_len 0
      --adaptive
      --fp16
      --dynamic-loss-scale
      --multi_gpu
      --cuda
      --pt
      --do_train
      --do_test
  type: grid
  max_trials: 12
  params:
    - name: pacing_function
      spec: discrete
      values: ['step','linear']
    - name: pacing_unit
      spec: discrete
      values: ['step','epoch']
    - name: a
      spec: discrete
      values: [0.06, 0.12, 0.18, 0.24]
    - name: b
      spec: discrete
      values: [1, 0.8, 0.6]

